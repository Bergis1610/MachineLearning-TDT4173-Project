{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emil B. Berglund - 529222 & Louis H. H. Linnerud - 539305, Team: ML er Bingo\n",
    "Public leaderboard rmsle = 0.67866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "0. Setup\n",
    "\n",
    "1. Exploratory data analysis\n",
    "\n",
    "2. Feature Engineering\n",
    "    - Build features\n",
    "        - Pure grunnkrets features\n",
    "        - Income features\n",
    "        - Age / Population features\n",
    "        - Household features\n",
    "        - Bus features\n",
    "        - store based features\n",
    "    - Combine features into one frame\n",
    "    - inspect features \n",
    "<br>\n",
    "<br>\n",
    "3. Models/Predictors\n",
    "    - LightGBM\n",
    "    - CatBoost\n",
    "    - XGBoost\n",
    "    - Random Forest Regressor\n",
    "    - H2O AutoML\n",
    "<br>\n",
    "<br>\n",
    "4. Model Interpretations\n",
    "    - Parameter importance\n",
    "    - feature importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________ _0. Setup_ ___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.ensemble as ensemble\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xg\n",
    "import featuretools as ft\n",
    "import category_encoders as ce\n",
    "import shap\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import log_loss, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from verstack import LGBMTuner, MeanTargetEncoder, OneHotEncoder\n",
    "from sklearn.neighbors import BallTree, KDTree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResultToFile(test_data, pred_data, nameOfFile='namelessSubmission'):\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = test_data['store_id']\n",
    "    submission['predicted'] = np.asarray(pred_data)\n",
    "    submission.to_csv('submissionFiles/'+ nameOfFile+'.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(dataSet, columns):\n",
    "    for column in columns:\n",
    "        dataSet.drop(column, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_retailers_with_0_revenue(dataSet):\n",
    "    dataSet.drop(dataSet[dataSet['revenue']==0.0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________ _2. Feature Engineering_ ___________\n",
    "Feature engingeering organized in the following order:\n",
    "- Build features\n",
    "    - Pure grunnkrets features\n",
    "    - Income features\n",
    "    - Age / Population features\n",
    "    - Household features\n",
    "    - Bus features\n",
    "    - store based features\n",
    "- Combine features into one frame\n",
    "- inspect features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "gk_incomes = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "gk_households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "gk_ages = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "buss_stopps = pd.read_csv('data/busstops_norway.csv')\n",
    "hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "\n",
    "grunnkrets.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "gk_incomes.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "gk_households.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "gk_ages.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "buss_stopps.drop_duplicates(subset=['busstop_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "municipality_area = grunnkrets.groupby('municipality_name').area_km2.transform('sum')\n",
    "grunnkrets['municipality_area'] = municipality_area\n",
    "gk_incomes.rename(columns = {'all_households':'income_gk'}, inplace = True)\n",
    "\n",
    "# income in district\n",
    "gk_districts = grunnkrets[['grunnkrets_id','district_name']]\n",
    "gk_incomes = pd.merge(gk_incomes, gk_districts, how='left', on='grunnkrets_id')\n",
    "\n",
    "income_district = gk_incomes.groupby(['district_name']).income_gk.transform('mean')\n",
    "gk_incomes['income_district'] = income_district\n",
    "gk_incomes['income_district_log'] = np.log1p(income_district)\n",
    "\n",
    "# Mean revenue in district\n",
    "gk_revenues = pd.DataFrame(stores_train[['grunnkrets_id','revenue']])\n",
    "gk_revenues.rename(columns = {'revenue':'revenue_gk'}, inplace = True)\n",
    "gk_revenues = gk_revenues.groupby('grunnkrets_id').revenue_gk.mean()\n",
    "gk_incomes = pd.merge(gk_incomes, gk_revenues, how='left', on='grunnkrets_id')\n",
    "#gk_incomes['revenue_gk'].fillna(0,inplace=True)\n",
    "gk_incomes['mean_rev_district'] = gk_incomes.groupby(['district_name']).revenue_gk.transform('mean')\n",
    "gk_incomes['mean_rev_district_log'] = np.log1p(gk_incomes['mean_rev_district'])\n",
    "\n",
    "# label income in district as low medium high\n",
    "upper = gk_incomes['income_district'].quantile(0.80)\n",
    "lower = gk_incomes['income_district'].quantile(0.20)\n",
    "maximum = gk_incomes['income_district'].max()\n",
    "gk_incomes['income_classification'] = pd.cut(gk_incomes.income_district, bins=[0,lower, upper, maximum], labels=[1,2,3]) \n",
    "\n",
    "# label income in gk as low medium high\n",
    "upper = gk_incomes['income_gk'].quantile(0.80)\n",
    "lower = gk_incomes['income_gk'].quantile(0.20)\n",
    "maximum = gk_incomes['income_gk'].max()\n",
    "gk_incomes['income_classification'] = pd.cut(gk_incomes.income_gk, bins=[0,lower, upper, maximum], labels=['low','medium','high'])\n",
    "\n",
    "\n",
    "# Merge in selected columns\n",
    "gk_area = grunnkrets[['grunnkrets_id','area_km2','municipality_area','municipality_name','district_name']]\n",
    "gk_ages = pd.merge(gk_ages, gk_area, how='left', on='grunnkrets_id')\n",
    "\n",
    "# Number of people features in grunnkrets\n",
    "tot_people_in_gk = np.sum(gk_ages.iloc[:,np.arange(2,93,1)], axis=1)\n",
    "gk_ages['tot_people_gk'] = tot_people_in_gk\n",
    "\n",
    "# Number of people features in municipality\n",
    "tot_people_in_municipality = gk_ages.groupby('municipality_name').tot_people_gk.transform('sum')\n",
    "gk_ages['tot_people_municipality'] = tot_people_in_municipality\n",
    "\n",
    "# Number of people features in district\n",
    "tot_people_in_district = gk_ages.groupby('district_name').tot_people_gk.transform('sum')\n",
    "gk_ages['tot_people_district'] = tot_people_in_district\n",
    "\n",
    "# People density gk\n",
    "gk_ages['people_density_gk'] = (gk_ages['tot_people_gk'] / gk_ages['area_km2'])\n",
    "gk_ages['people_density_gk_log'] = np.log1p(gk_ages['tot_people_gk'] / gk_ages['area_km2'])\n",
    "\n",
    "# People density municipality\n",
    "gk_ages['people_density_municipality'] = (gk_ages['tot_people_municipality'] / gk_ages['municipality_area'])\n",
    "gk_ages['people_density_municipality_log'] = np.log1p(gk_ages['tot_people_municipality'] / gk_ages['municipality_area'])\n",
    "\n",
    "# People density district\n",
    "district_area_km2 = gk_ages.groupby('municipality_name').area_km2.transform('sum')\n",
    "gk_ages['district_area_km2'] = district_area_km2\n",
    "\n",
    "gk_ages['people_density_district'] = (gk_ages['tot_people_district'] / gk_ages['district_area_km2'])\n",
    "gk_ages['people_density_district_log'] = np.log1p(gk_ages['people_density_district'])\n",
    "\n",
    "# City name\n",
    "gk_ages['city'] = \"none\"\n",
    "col_idx = gk_ages.columns.get_loc('city')\n",
    "for mun in gk_ages['municipality_name'].unique():\n",
    "        gk_ages.iloc[gk_ages[(gk_ages['municipality_name']==mun) & (gk_ages['tot_people_municipality']>100000)].index,[col_idx]] = mun\n",
    "\n",
    "gk_municipalities = grunnkrets[['grunnkrets_id','municipality_name']]\n",
    "\n",
    "# Number of house holds grunnkrets level\n",
    "gk_households['nb_households_gk']  = np.sum(gk_households.iloc[:,np.arange(2,10,1)], axis=1)\n",
    "gk_households['nb_households_gk_log']  = np.log1p(gk_households['nb_households_gk'])\n",
    "\n",
    "# Number of house holds municipality level\n",
    "gk_households = pd.merge(gk_households, gk_municipalities, how='left', on='grunnkrets_id')\n",
    "nb_housholds_municipality = gk_households.groupby('municipality_name').nb_households_gk.transform('sum')\n",
    "gk_households['nb_households_municipality'] = nb_housholds_municipality\n",
    "gk_households['nb_households_municipality_log'] = np.log1p(nb_housholds_municipality)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for dividing up the plaace hierarchy into separate categories\n",
    "def add_hierarchy(stores):\n",
    "    chosen_hiercs = hierarchy[['plaace_hierarchy_id','lv1_desc','lv2_desc','lv3_desc']]\n",
    "    stores_with_hierarchy = pd.merge(stores,chosen_hiercs, how='inner', on='plaace_hierarchy_id')\n",
    "    stores['lv1_desc'] = stores_with_hierarchy['lv1_desc']\n",
    "    stores['lv2_desc'] = stores_with_hierarchy['lv2_desc']\n",
    "    stores['lv3_desc'] = stores_with_hierarchy['lv3_desc']\n",
    "\n",
    "# Helper function for adding lat and lon to busses\n",
    "def add_latlong(buss):\n",
    "    buss['lat'] = 0.0\n",
    "    buss['lon'] = 0.0\n",
    "    lonList = []\n",
    "    latList = [] \n",
    "    for index, row in buss.iterrows():\n",
    "        lon = row['geometry']\n",
    "        lon = lon[6:]\n",
    "        lon = lon.replace(')','')\n",
    "        \n",
    "        lat = lon.split()[1]\n",
    "        lon = lon.split()[0]\n",
    "        lon = float(lon)\n",
    "        lat = float(lat) \n",
    "\n",
    "        lonList.append(lon)\n",
    "        latList.append(lat)\n",
    "        \n",
    "    buss['lon'] = np.array(lonList)\n",
    "    buss['lat'] = np.array(latList)\n",
    "\n",
    "# Add lat lon to bus_stops\n",
    "add_latlong(buss_stopps) \n",
    "\n",
    "# Add bus features to data frame\n",
    "def add_buss_feats(stores,r1=0.3,r2=5,r3=10):\n",
    "    k_neighbors =1200\n",
    "    minutes = 60 #minutes\n",
    "    nautic = 1.852 #km\n",
    "    \n",
    "    buss_stopps[[\"lat_rad\", \"lon_rad\"]] = np.deg2rad(buss_stopps[[\"lat\", \"lon\"]])\n",
    "    stores[[\"lat_rad\", \"lon_rad\"]] = np.deg2rad(stores[[\"lat\", \"lon\"]])\n",
    "    \n",
    "    # All bus stops\n",
    "    tree = BallTree(buss_stopps[[\"lat_rad\", \"lon_rad\"]].values, metric='haversine')\n",
    "\n",
    "    distances, indices = tree.query(stores[[\"lat_rad\", \"lon_rad\"]], k = k_neighbors)\n",
    "    distances = np.rad2deg(distances) # convert back to radians\n",
    "    distances_km = distances*minutes*nautic\n",
    "    nb_stops_r1 = np.count_nonzero(distances_km < r1, axis=1)\n",
    "    nb_stops_r2 = np.count_nonzero(distances_km < r2, axis=1)\n",
    "    nb_stops_r3 = np.count_nonzero(distances_km < r3, axis=1)\n",
    "    dist_closest_bus = np.min(distances_km,axis=1)\n",
    "    \n",
    "    stores['dist_closest_bus'] = dist_closest_bus\n",
    "    stores['dist_closest_bus_transformed'] = np.log1p(dist_closest_bus*10000)\n",
    "    stores['nb_stops_r1'] = nb_stops_r1\n",
    "    stores['nb_stops_r2'] = nb_stops_r2\n",
    "    stores['nb_stops_r3'] = nb_stops_r3\n",
    "    \n",
    "    # Important bus stops\n",
    "    important_buss_stops = buss_stopps[buss_stopps['importance_level'] == 'Nasjonalt knutepunkt']\n",
    "    \n",
    "    tree = BallTree(important_buss_stops[[\"lat_rad\", \"lon_rad\"]].values, metric='haversine')\n",
    "\n",
    "    distances, indices = tree.query(stores[[\"lat_rad\", \"lon_rad\"]], k = 2) #2 because we eperienced some unstability with 1\n",
    "    distances = np.rad2deg(distances) # convert back to radians\n",
    "    distances_km = distances*minutes*nautic\n",
    "    dist_closest_important_stop = np.min(distances_km,axis=1)\n",
    "    \n",
    "    stores['dist_closest_important_stop'] = dist_closest_important_stop\n",
    "    stores[stores['dist_closest_important_stop'] < 0.00001] = np.max(dist_closest_important_stop,axis=0)#.replace(to_replace = 0,  value= np.max(dist_closest_important_stop), inplace=True)\n",
    "    \n",
    "    stores['dist_closest_important_stop_transformed'] = np.log1p(dist_closest_important_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean rev within each chain\n",
    "stores_train['chain_name'].fillna('0', inplace=True)\n",
    "\n",
    "#trying to avoid overfitting small chains\n",
    "for chain in stores_train['chain_name'].unique():\n",
    "    if stores_train[stores_train['chain_name']==chain].shape[0] < 3:\n",
    "        stores_train[stores_train['chain_name']==chain].chain_name = 'smallChain'\n",
    "\n",
    "chain_mean_rev =  stores_train.groupby(['chain_name']).revenue.transform('mean')\n",
    "stores_train['mean_chain_rev'] = chain_mean_rev\n",
    "\n",
    "chain_rev_df = stores_train[['chain_name','mean_chain_rev']]\n",
    "chain_rev_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain_mean_rev =  stores_train.groupby(['chain_name']).revenue.transform('mean')\n",
    "stores_train['mean_chain_rev'] = chain_mean_rev\n",
    "\n",
    "chain_rev_df = stores_train[['chain_name','mean_chain_rev']]\n",
    "chain_rev_df.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_aggregate_columns (stores):\n",
    "    # Load in all datasets containg stores to make the most accurate features\n",
    "    stores_ex = pd.read_csv('data/stores_extra.csv')\n",
    "    stores_tr = pd.read_csv('data/stores_train.csv')\n",
    "    stores_te = pd.read_csv('data/stores_test.csv')\n",
    "    all_stores = stores_ex.copy()\n",
    "    all_stores = all_stores.append(stores_tr)\n",
    "    all_stores = all_stores.append(stores_te)\n",
    "    \n",
    "    minutes=60\n",
    "    nautic=1.852 #km  \n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    # has chain\n",
    "    stores['chain_name'] = stores['chain_name'].fillna('0')\n",
    "    stores['has_chain'] = stores['chain_name'].apply(lambda x: 1 if x !='0' else 0)\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    # Number of similar stores in municipality\n",
    "    gk_municipalities = grunnkrets[['grunnkrets_id','municipality_name']]\n",
    "    stores = pd.merge(stores, gk_municipalities, how='left', on='grunnkrets_id')\n",
    "    \n",
    "    nb_similar_stores_municipality = stores.groupby(['municipality_name','plaace_hierarchy_id']).plaace_hierarchy_id.transform('count')\n",
    "    stores['nb_similar_stores_municipality'] = nb_similar_stores_municipality\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    # Closest competitors\n",
    "    all_stores[[\"lat_rad\", \"lon_rad\"]] = np.deg2rad(all_stores[[\"lat\", \"lon\"]])\n",
    "    stores[[\"lat_rad\", \"lon_rad\"]] = np.deg2rad(stores[[\"lat\", \"lon\"]])\n",
    "    \n",
    "    stores['dist_closest_comp_km'] = 0\n",
    "    \n",
    "    for store_type in stores['plaace_hierarchy_id'].unique():\n",
    "        \n",
    "        temp_all_df = all_stores.loc[all_stores['plaace_hierarchy_id']==store_type]\n",
    "        temp_target_df = stores.loc[stores['plaace_hierarchy_id']==store_type, ['store_id','lat_rad','lon_rad']]\n",
    "        \n",
    "        if temp_target_df.shape[0] > 1:\n",
    "            tree = BallTree(temp_all_df[[\"lat_rad\", \"lon_rad\"]].values, metric='haversine')\n",
    "            \n",
    "            distances, indices = tree.query(temp_target_df[[\"lat_rad\", \"lon_rad\"]], k = 2)\n",
    "            \n",
    "            #distances = np.rad2deg(distances) # convert back to radians #TODO this has been forgotten, check if it improves result here and at the above one \n",
    "\n",
    "            distances_km = distances*minutes*nautic\n",
    "            \n",
    "            temp_target_df[\"dist_closest_comp_km_temp\"] =np.max(distances_km, axis=1)\n",
    "            \n",
    "            temp_target_df.drop(['lat_rad','lon_rad'],axis=1, inplace=True)\n",
    "            stores = pd.merge(stores, temp_target_df, how='left', on='store_id')\n",
    "            stores['dist_closest_comp_km_temp'].fillna(0, inplace=True)\n",
    "            \n",
    "            stores['dist_closest_comp_km'] = stores['dist_closest_comp_km'] + stores['dist_closest_comp_km_temp']\n",
    "            stores.drop('dist_closest_comp_km_temp', axis=1, inplace=True)\n",
    "    \n",
    "    # closest competitor transformed\n",
    "    stores['dist_closest_comp_km_transform'] = stores['dist_closest_comp_km']\n",
    "    stores['dist_closest_comp_km_transform'].replace(to_replace = 0,  method='ffill', inplace=True)\n",
    "    stores['dist_closest_comp_km_transform'] = np.log1p(stores['dist_closest_comp_km_transform'])#*10000)\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    # Closest competitor in chain\n",
    "    stores['dist_closest_chain_km'] = 0\n",
    "\n",
    "    for chain in stores['chain_name'].unique():\n",
    "        \n",
    "        temp_all_df = all_stores.loc[all_stores['chain_name']==chain]\n",
    "        temp_target_df = stores.loc[stores['chain_name']==chain, ['store_id','lat_rad','lon_rad']]\n",
    "        \n",
    "        if temp_target_df.shape[0] > 1 and temp_all_df.shape[0] > 1 :\n",
    "            tree = BallTree(temp_all_df[[\"lat_rad\", \"lon_rad\"]].values, metric='haversine')\n",
    "            \n",
    "            distances, indices = tree.query(temp_target_df[[\"lat_rad\", \"lon_rad\"]], k = 2)\n",
    "\n",
    "            distances_km = distances*minutes*nautic\n",
    "            \n",
    "            temp_target_df[\"dist_closest_chain_km_temp\"] =np.max(distances_km, axis=1)\n",
    "            \n",
    "            temp_target_df.drop(['lat_rad','lon_rad'],axis=1, inplace=True)\n",
    "            stores = pd.merge(stores, temp_target_df, how='left', on='store_id')\n",
    "            stores['dist_closest_chain_km_temp'].fillna(0, inplace=True)\n",
    "            \n",
    "            stores['dist_closest_chain_km'] = stores['dist_closest_chain_km'] + stores['dist_closest_chain_km_temp']\n",
    "            stores.drop('dist_closest_chain_km_temp', axis=1, inplace=True)\n",
    "\n",
    "    # closest chain transformed\n",
    "    stores['dist_closest_chain_km_transform'] = stores['dist_closest_chain_km']\n",
    "    stores['dist_closest_chain_km_transform'].replace(to_replace = 0,  method='ffill', inplace=True)\n",
    "    stores['dist_closest_chain_km_transform'] = np.log1p(stores['dist_closest_chain_km_transform'])#*10000)\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    # Number of stores within range\n",
    "    tree = BallTree(all_stores[[\"lat_rad\", \"lon_rad\"]].values, metric='haversine')\n",
    "    \n",
    "    distances , indices = tree.query(stores[[\"lat_rad\", \"lon_rad\"]], k = 1200)\n",
    "    distances = np.rad2deg(distances) # convert back to radians\n",
    "    distances_km = distances*minutes*nautic\n",
    "    \n",
    "    r_1 = 0.1 #km\n",
    "    r_2 = 0.5 #km\n",
    "    r_3 = 1 #km\n",
    "    r_4 = 5 #km\n",
    "    r_5 = 10 #km\n",
    "    \n",
    "    stores['nb_stores_r1']  = np.count_nonzero(distances_km < r_1, axis=1)\n",
    "    stores['nb_stores_r2']  = np.count_nonzero(distances_km < r_2, axis=1)\n",
    "    stores['nb_stores_r3']  = np.count_nonzero(distances_km < r_3, axis=1)\n",
    "    stores['nb_stores_r4']  = np.count_nonzero(distances_km < r_4, axis=1)\n",
    "    stores['nb_stores_r5']  = np.count_nonzero(distances_km < r_5, axis=1)\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    # Number of similar stores within radius\n",
    "    radiuses = [10,5,1,0.5,0.1]\n",
    "    for r in radiuses:\n",
    "        stores[f\"nb_of_close_competitors_{r}\"] = 0\n",
    "        \n",
    "        for store_type in stores['plaace_hierarchy_id'].unique():\n",
    "            \n",
    "            temp_all_df = all_stores.loc[all_stores['plaace_hierarchy_id']==store_type]\n",
    "            temp_target_df = stores.loc[stores['plaace_hierarchy_id']==store_type, ['store_id','lat_rad','lon_rad']]\n",
    "            \n",
    "            if temp_target_df.shape[0] > 1:\n",
    "                k_neighs = temp_target_df.shape[0]\n",
    "                \n",
    "                tree = BallTree(temp_all_df[[\"lat_rad\", \"lon_rad\"]].values, metric='haversine')\n",
    "                \n",
    "                distances, indices = tree.query(temp_target_df[[\"lat_rad\", \"lon_rad\"]], k = (k_neighs-1))\n",
    "                distances = np.rad2deg(distances)\n",
    "                distances_km = distances*minutes*nautic\n",
    "                distances_km[distances_km > r] = 0\n",
    "                \n",
    "                temp_target_df[f\"nb_of_close_competitors_temp_{r}\"] =np.count_nonzero(distances_km, axis=1)\n",
    "                \n",
    "                temp_target_df.drop(['lat_rad','lon_rad'],axis=1, inplace=True)\n",
    "                stores = pd.merge(stores, temp_target_df, how='left', on='store_id')\n",
    "                stores[f\"nb_of_close_competitors_temp_{r}\"].fillna(0, inplace=True)\n",
    "                \n",
    "                stores[f\"nb_of_close_competitors_{r}\"] = stores[f\"nb_of_close_competitors_{r}\"] + stores[f\"nb_of_close_competitors_temp_{r}\"]\n",
    "                stores.drop(f\"nb_of_close_competitors_temp_{r}\", axis=1, inplace=True)\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    \n",
    "    \n",
    "    remove_columns(stores,['municipality_name'])# gets added later\n",
    "    return stores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all features into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_selected_columns(df, include_bad_columns=False):\n",
    "    # Add all columns\n",
    "    df = self_aggregate_columns(df)\n",
    "    add_buss_feats(df)\n",
    "    add_hierarchy(df)\n",
    "    gk = grunnkrets[['grunnkrets_id','municipality_name', 'district_name']]\n",
    "    gk_i = gk_incomes[['grunnkrets_id','income_gk','income_classification','income_district','mean_rev_district','mean_rev_district_log','income_district_log','single_parent_with_children']]\n",
    "    gk_h = gk_households[['grunnkrets_id','nb_households_gk','nb_households_municipality','nb_households_gk_log','nb_households_municipality_log']]\n",
    "    gk_a = gk_ages[['grunnkrets_id','tot_people_gk','tot_people_district','tot_people_municipality','people_density_gk_log','people_density_municipality_log','people_density_gk','people_density_municipality','city','people_density_district','people_density_district_log']]\n",
    "    chain_rev = chain_rev_df[['chain_name','mean_chain_rev']]\n",
    "    \n",
    "    concat = pd.merge(df, gk, how='left', on='grunnkrets_id')\n",
    "    concat = pd.merge(concat, gk_i, how='left', on='grunnkrets_id')\n",
    "    concat = pd.merge(concat, gk_h, how='left', on='grunnkrets_id')\n",
    "    concat = pd.merge(concat, gk_a, how='left', on='grunnkrets_id')\n",
    "    concat = pd.merge(concat, chain_rev, how='left', on='chain_name')\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    # NaN handling\n",
    "    concat['income_gk'].fillna(450000, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    concat['municipality_name'].fillna('0', inplace=True)\n",
    "    concat['income_classification'].fillna('medium', inplace=True)\n",
    "    concat['nb_similar_stores_municipality'].fillna(0, inplace=True)\n",
    "    \n",
    "    concat['city'].fillna('none', inplace=True)\n",
    "    concat['people_density_district'].fillna(concat['people_density_district'].dropna().mean(), inplace=True)#the mean value\n",
    "    concat['people_density_district_log'].fillna(concat['people_density_district_log'].dropna().mean(), inplace=True)        \n",
    "    concat['district_name'].fillna('none', inplace=True)\n",
    "    concat['mall_name'].fillna('none', inplace=True)\n",
    "    concat['income_district'].fillna(466285.26671004744, inplace=True) #replace with mean value\n",
    "    concat['mean_rev_district'].fillna(8.20564984134999, inplace=True) #replace with mean value\n",
    "    concat['tot_people_district'].fillna(concat['tot_people_district'].dropna().mean(), inplace=True)\n",
    "    concat['tot_people_municipality'].fillna(concat['tot_people_municipality'].dropna().mean(), inplace=True)        \n",
    "    concat['people_density_gk_log'].fillna(concat['people_density_gk_log'].dropna().mean(), inplace=True)\n",
    "    concat['people_density_municipality_log'].fillna(concat['people_density_municipality_log'].dropna().mean(), inplace=True)\n",
    "    concat['people_density_gk'].fillna(concat['people_density_gk'].dropna().mean(), inplace=True)\n",
    "    concat['people_density_municipality'].fillna(concat['people_density_municipality'].dropna().mean(), inplace=True)\n",
    "    concat['mean_chain_rev'].fillna(4.29, inplace=True)\n",
    "    concat['single_parent_with_children'].fillna(concat['single_parent_with_children'].mean(), inplace=True)\n",
    "    concat['single_parent_with_children'].mean()\n",
    "    concat['grunnkrets_id'] = concat['grunnkrets_id'].astype(np.int0)\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    # Select columns we want to return\n",
    "    if not include_bad_columns:\n",
    "        remove_columns(concat, [\n",
    "                                'plaace_hierarchy_id',\n",
    "                                'store_id',\n",
    "                                'store_name',\n",
    "                                'year',\n",
    "                                'address',\n",
    "                                'lat',\n",
    "                                'lon',\n",
    "                                'lat_rad',\n",
    "                                'lon_rad',\n",
    "                                'has_chain',\n",
    "                                'municipality_name',\n",
    "                                'district_name',\n",
    "                                'income_district',\n",
    "                                'income_district_log',# use unlogged column instead due to weird distribution somehow\n",
    "                                'single_parent_with_children',\n",
    "                                'mean_rev_district_log', #make things alot worse on kaggle\n",
    "                                'mean_rev_district',\n",
    "                                'income_classification',\n",
    "                                'nb_similar_stores_municipality', # radius > Munc\n",
    "                                'nb_households_gk', # radius > grunnkrets\n",
    "                                'nb_households_gk_log',\n",
    "                                'nb_households_municipality', #very ugly distributed\n",
    "                                'nb_households_municipality_log',\n",
    "                                'tot_people_gk',\n",
    "                                'tot_people_district',\n",
    "                                'tot_people_municipality', # should be removed, very poor distribution\n",
    "                                'people_density_gk', # this distributibution is suuuuperweird compared in the different sets..\n",
    "                                'people_density_gk_log',\n",
    "                                'people_density_municipality',\n",
    "                                'people_density_municipality_log',\n",
    "                                'dist_closest_bus_transformed',\n",
    "                                'dist_closest_important_stop_transformed',\n",
    "                                'dist_closest_comp_km_transform',\n",
    "                                'dist_closest_chain_km_transform',\n",
    "                                'people_density_district',\n",
    "                                'people_density_district_log'\n",
    "                                ])\n",
    "    #______________________________________________________________________________________________________________________________________________________\n",
    "    \n",
    "    return concat\n",
    "#Chosen columns are below\n",
    "#'chain_name',\n",
    "#'mall_name',\n",
    "#'income_gk',\n",
    "#'sales_channel_name',\n",
    "#'lv1_desc',\n",
    "#'lv2_desc',\n",
    "#'lv3_desc',\n",
    "#'grunnkrets_id',\n",
    "#'dist_closest_bus',\n",
    "#'nb_stops_r1',\n",
    "#'nb_stops_r2',\n",
    "#'nb_stops_r3',\n",
    "#'dist_closest_important_stop',\n",
    "#'nb_stores_r1',\n",
    "#'nb_stores_r2',\n",
    "#'nb_stores_r3',\n",
    "#'nb_stores_r4',\n",
    "#'nb_stores_r5',\n",
    "#'mean_chain_rev',\n",
    "#'dist_closest_comp_km',\n",
    "#'dist_closest_chain_km',\n",
    "#'city',\n",
    "#Bokmerke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' le = LabelEncoder()\\nsns.set(font_scale=0.9)\\nselected_cols = [\\n                 # change \"in stores_test\" below with selected_cols to only show the ones you wanna plot\\n                ]\\nfor col_name in stores_test:\\n    \\n    if col_name ==\\'income_classification\\':\\n        stores_test[col_name] = le.fit_transform(stores_test[col_name])\\n        stores_train[col_name] = le.fit_transform(stores_train[col_name])\\n            \\n    elif stores_test[col_name].dtypes != \\'object\\':\\n        stores_test[col_name] = stores_test[col_name]\\n        stores_train[col_name] = stores_train[col_name]\\n        \\n    else:\\n        stores_test[col_name] = le.fit_transform(stores_test[col_name])\\n        stores_train[col_name] = le.fit_transform(stores_train[col_name])\\n        \\n    \\n    fig, (ax1, ax2) = plt.subplots(figsize=(15, 5), ncols=2, dpi=100)\\n    sns.distplot(stores_test[col_name], ax=ax1);\\n    ax1.set_title(f\"Distribution {col_name} test set\");\\n    sns.distplot(stores_train[col_name], ax=ax2);\\n    ax2.set_title(f\"Distribution {col_name} train set\");\\n    plt.show()\\n        \\n    break # comment out to show all plots '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" le = LabelEncoder()\n",
    "sns.set(font_scale=0.9)\n",
    "selected_cols = [\n",
    "                 # change \"in stores_test\" below with selected_cols to only show the ones you wanna plot\n",
    "                ]\n",
    "for col_name in stores_test:\n",
    "    \n",
    "    if col_name =='income_classification':\n",
    "        stores_test[col_name] = le.fit_transform(stores_test[col_name])\n",
    "        stores_train[col_name] = le.fit_transform(stores_train[col_name])\n",
    "            \n",
    "    elif stores_test[col_name].dtypes != 'object':\n",
    "        stores_test[col_name] = stores_test[col_name]\n",
    "        stores_train[col_name] = stores_train[col_name]\n",
    "        \n",
    "    else:\n",
    "        stores_test[col_name] = le.fit_transform(stores_test[col_name])\n",
    "        stores_train[col_name] = le.fit_transform(stores_train[col_name])\n",
    "        \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(15, 5), ncols=2, dpi=100)\n",
    "    sns.distplot(stores_test[col_name], ax=ax1);\n",
    "    ax1.set_title(f\"Distribution {col_name} test set\");\n",
    "    sns.distplot(stores_train[col_name], ax=ax2);\n",
    "    ax2.set_title(f\"Distribution {col_name} train set\");\n",
    "    plt.show()\n",
    "        \n",
    "    break # comment out to show all plots \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________ _3. Machine Learning Models and Predictions_ ___________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode LGBM\n",
    "def convert_DType_LGBM(dFrame):\n",
    "    le = LabelEncoder()\n",
    "    X = pd.DataFrame()\n",
    "    \n",
    "    for col_name in dFrame:\n",
    "        if dFrame[col_name].dtypes == 'object':\n",
    "            X[col_name] = dFrame[col_name].astype('category')\n",
    "            \n",
    "        elif col_name == 'has_chain':\n",
    "            X[col_name] = dFrame[col_name].astype('category')\n",
    "        \n",
    "        else:\n",
    "            X[col_name] = dFrame[col_name]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Catboost\n",
    "def convert_DType_CatBoost_2(train_frame,test_frame=None):\n",
    "    X = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "    \n",
    "    for col_name in train_frame:\n",
    "        if col_name == 'grunnkrets_id':\n",
    "            X[col_name] = train_frame[col_name].astype(np.int0)\n",
    "            X[col_name] = train_frame[col_name].astype('category')\n",
    "        \n",
    "        elif train_frame[col_name].dtypes == 'object':\n",
    "            X[col_name] = train_frame[col_name].astype('category')\n",
    "        \n",
    "        else:\n",
    "            X[col_name] = train_frame[col_name]\n",
    "        \n",
    "    for col_name in test_frame:\n",
    "        if col_name == 'grunnkrets_id':\n",
    "            test[col_name] = test_frame[col_name].astype(np.int0)\n",
    "            test[col_name] = test_frame[col_name].astype('category')\n",
    "        \n",
    "        elif test_frame[col_name].dtypes == 'object':\n",
    "            test[col_name] = test_frame[col_name].astype('category')\n",
    "        \n",
    "        else:\n",
    "            test[col_name] = test_frame[col_name]\n",
    "    \n",
    "    return X, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    stores_train = pd.read_csv('data/stores_train.csv')\n",
    "    stores_test = pd.read_csv('data/stores_test.csv')\n",
    "\n",
    "    # Add features\n",
    "    stores_train = add_selected_columns(stores_train, include_bad_columns=False)\n",
    "    #if include_submission_set:\n",
    "    stores_test = add_selected_columns(stores_test, include_bad_columns=False)\n",
    "\n",
    "    # Divide data into train and test set\n",
    "    \n",
    "    x_train = stores_train.drop('revenue', axis=1)\n",
    "    \n",
    "    y_train = stores_train['revenue']\n",
    "    y_train=np.log1p(y_train) #log transform revenue\n",
    "    \n",
    "    return x_train, y_train, stores_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sales_channel_name', 'grunnkrets_id', 'chain_name', 'mall_name',\n",
      "       'dist_closest_comp_km', 'dist_closest_chain_km', 'nb_stores_r1',\n",
      "       'nb_stores_r2', 'nb_stores_r3', 'nb_stores_r4', 'nb_stores_r5',\n",
      "       'nb_of_close_competitors_10', 'nb_of_close_competitors_5',\n",
      "       'nb_of_close_competitors_1', 'nb_of_close_competitors_0.5',\n",
      "       'nb_of_close_competitors_0.1', 'dist_closest_bus', 'nb_stops_r1',\n",
      "       'nb_stops_r2', 'nb_stops_r3', 'dist_closest_important_stop', 'lv1_desc',\n",
      "       'lv2_desc', 'lv3_desc', 'income_gk', 'city', 'mean_chain_rev'],\n",
      "      dtype='object')\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=4.139004254818685, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=4.139004254818685\n",
      "[LightGBM] [Warning] num_threads is set=6, n_jobs=-1 will be ignored. Current value: num_threads=6\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#x_train, x_test, y_train, y_test, test = get_data(test_size=0.0001)\n",
    "x_train, y_train, test = get_data()\n",
    "print(x_train.columns)\n",
    "\n",
    "# Convert to approperiate dtypes\n",
    "LGBM_x_train_1 = convert_DType_LGBM(x_train)\n",
    "LGBM_test_1 = convert_DType_LGBM(test)\n",
    "categorical_features = [f for f in LGBM_x_train_1.columns if LGBM_x_train_1[f].dtype == 'category']\n",
    "\n",
    "#CB_x_train,CB_test = convert_DType_CatBoost(x_train, y_train, test)\n",
    "CB_x_train, CB_test = convert_DType_CatBoost_2(x_train, test)\n",
    "categorical_features_indices = np.where((CB_x_train.dtypes == 'object') | (CB_x_train.dtypes == str) | (CB_x_train.dtypes == 'category'))[0]\n",
    "\n",
    "# LGBM_1\n",
    "# parameters found through verstack, but learning rate and n_iterations have been modified through testing of our own\n",
    "LGBM_parameters = {'learning_rate': 0.005, 'num_leaves': 253, 'colsample_bytree': 0.8609727402803514, 'subsample': 0.8100744538922787, 'verbosity': -1, 'random_state': 42, 'objective': 'regression', 'metric': 'l2', 'num_threads': 6, 'reg_alpha': 1.0025757876059077e-06, 'min_sum_hessian_in_leaf': 4.139004254818685, 'reg_lambda': 0.0018151441142073164, 'n_estimators': 500}\n",
    "#LGBM_parameters = {}\n",
    "\n",
    "\n",
    "LGBM_model_1 = LGBMRegressor(**LGBM_parameters)\n",
    "LGBM_model_1.fit(LGBM_x_train_1, y_train, categorical_feature=categorical_features)\n",
    "LGBM_pred_1 = LGBM_model_1.predict(LGBM_test_1)\n",
    "#LGBM_pred=np.expm1(LGBM_pred) #invert log transform\n",
    "\n",
    "# Catboost\n",
    "CB_parameters = {'depth': 12, 'iterations': 3000, 'learning_rate': 0.005}\n",
    "#CB_parameters = {}\n",
    "CB_model = cb.CatBoostRegressor(loss_function='RMSE', **CB_parameters, silent=True)\n",
    "CB_model.fit(CB_x_train,y_train, cat_features=categorical_features_indices)\n",
    "CB_pred = CB_model.predict(CB_test)\n",
    "#CB_pred = np.expm1(CB_pred)\n",
    "\n",
    "# Aggregate result\n",
    "PREDICTION= np.expm1(((CB_pred*0.7)+(LGBM_pred_1*0.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8577 entries, 0 to 8576\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   id         8577 non-null   object \n",
      " 1   predicted  8577 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 134.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#write the predicition to file\n",
    "stores_test = pd.read_csv('data/stores_test.csv')\n",
    "writeResultToFile(stores_test, PREDICTION, \"CB_LGBM_FINAL_2_1\")\n",
    "\n",
    "# Verify format of submission file\n",
    "submissionVery = pd.read_csv('submissionFiles/CB_LGBM_FINAL_2_1.csv')\n",
    "submissionVery.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the Short Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
