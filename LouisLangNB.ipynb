{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Notebook - Active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emil B. Berglund - 529222 & Louis H. H. Linnerud - 539305, Team: Noe Lættis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "1. Exploratory data analysis\n",
    "2. Feature Engineering \n",
    "3. Models/Predictors\n",
    "    - LightGBM\n",
    "    - Random Forest Regressor\n",
    "4. Model Interpretations\n",
    "    - feature importance\n",
    "5. Improved models (possibly)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________ _0. Setup_ ___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.ensemble as ensemble\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import featuretools as ft\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.metrics import log_loss, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from verstack import LGBMTuner, MeanTargetEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "#from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResultToFile(test_data, pred_data, nameOfFile='namelessSubmission'):\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = test_data['store_id']\n",
    "    submission['predicted'] = np.asarray(pred_data)\n",
    "    submission.to_csv('submissionFiles/'+ nameOfFile+'.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return metrics.mean_squared_log_error(y_true, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________ _1. Exploratory Data Analysis_ ___________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Notes\n",
    "- [x] Search domain knowledge\n",
    "- [x] Check if the data is intuitive\n",
    "- [ ] Understand how the data was generated\n",
    "- [x] Explore individual features\n",
    "    - [x] Agencies\n",
    "    - [x] stores with 0 revenue\n",
    "    - [x] food and drink stores and grovery stores\n",
    "- [x] Explore pairs and groups\n",
    "    - [x] Store type vs revenue\n",
    "    - [x] Geo position of stores in train and test set\n",
    "    - [x] Revenue based on geo position\n",
    "- [x] Clean up features\n",
    "    - [x] remove 2016\n",
    "    - [x] remove outliers\n",
    "    - [x] remove 0 revenue rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain Knowledge\n",
    "\n",
    "Retailers obviously earn their revenue from sales, different retailers sell different products to different customers. Different products have different margins and number of sales, directly impacting the revenue. Number of sales most likely have a high correlation with number of costumers, areas with a high population density will therefor most likely have a higher number of customers, impacting number of sales and then impacting the revenue. Therefor retailer type and geographical position most likely have a high impact on revenue. Only knowing those two attributes can be a good pin pointer, but not necessary enough as described in this article: https://carto.com/blog/retail-revenue-prediction-data-science/. Area infrastructure, retailer reputation, market competition, inventory managements, customer type, sales strategy and a lot more factors impact revenue and makes this problem complex. further reading on some of these factors: https://smallbusiness.chron.com/calculate-percentage-profit-markups-business-60099.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the data intuitiv?\n",
    "\n",
    "As can be seen below, data is organized in rows, where each row represents a single retailer with its relevant attributes and revenue. The stores train, extra and test data is intuitiv.\n",
    "\n",
    "The different grunnkrets data was not super intuitv before some exploration was done, the same grunnkrets_id appeard more than once, but we shortly realized that this is because the measurment (example: average income) is done twice, once in 2015 and once in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report = ProfileReport(stores_train)\n",
    "#report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore individual features and pairs and groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore revenue based on store type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stores_train.plaace_hierarchy_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaace_hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "stores_with_hierarchy = stores_train.merge(plaace_hierarchy, how='left', on='plaace_hierarchy_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.gcf().set_dpi(600)\n",
    "plt.xticks(rotation=90)\n",
    "sns.violinplot(x='lv2_desc',y='revenue',data=stores_with_hierarchy).set_title(\"Revenue on store type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further exploration of agencie store type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy[stores_with_hierarchy[\"lv2_desc\"]==\"Agencies\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further exploration of \"Food and drink\" type stores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.violinplot(x='lv3_desc',y='revenue',data=stores_with_hierarchy[stores_with_hierarchy[\"lv2_desc\"]==\"Food and drinks\"]).set_title(\"Food and drinks violin plot\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore retailers with NaN, 0 or negative revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy[stores_with_hierarchy[\"revenue\"]==0.0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy[stores_with_hierarchy[\"revenue\"] < 0.0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy[stores_with_hierarchy[\"revenue\"]== np.nan].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All retailers and their cooresponding revenue, the plot is to visually check for outliers, clearly there are som outliers as can be seen in the long tail to the right of the major distribution.\n",
    "The data is clearly positively skewed, confirmed by the skew number printed above the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_log = pd.DataFrame()\n",
    "rev_log['revenue_log'] = np.log1p(stores_train['revenue'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(15, 5), ncols=2, dpi=100)\n",
    "sns.distplot(stores_train['revenue'], ax=ax1);\n",
    "ax1.set_title('Distribution revenue');\n",
    "sns.distplot(rev_log['revenue_log'], ax=ax2);\n",
    "ax2.set_title('Distribution of revenue after log transform');\n",
    "\n",
    "print(f\"raw data skew: {stores_train['revenue'].skew()}\")\n",
    "print(f\"log transform skew: {rev_log['revenue_log'].skew()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns function - example: year is a const value and has no effect on the end result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(dataSet, columns):\n",
    "    for column in columns:\n",
    "        dataSet.drop(column, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns(stores_train,['year'])\n",
    "stores_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove retailers with 0 revenue function - might be handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_retailers_with_0_revenue(dataSet):\n",
    "    dataSet.drop(dataSet[dataSet['revenue']==0.0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers\n",
    "\n",
    "Plotting all retailers based on storetype before and after trimming to confirm that outliers actually has been removed\n",
    "\n",
    "Below is before trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_type in stores_with_hierarchy['lv2_desc'].unique():\n",
    "    plt.figure(figsize=(12,2))\n",
    "    sns.violinplot(x='lv3_desc',y='revenue',data=stores_with_hierarchy[stores_with_hierarchy[\"lv2_desc\"]==store_type]).set_title(f\"{store_type} violin plot\")\n",
    "    plt.show()\n",
    "    break #comment out for exploring more store types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cap-outliers-function for the relationship between store type and revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_storeType_vs_revenue(stores, lower, upper):\n",
    "    col_idx = stores.columns.get_loc('revenue')\n",
    "    for store_type in stores['plaace_hierarchy_id'].unique():\n",
    "        data = stores[stores['plaace_hierarchy_id']==store_type]\n",
    "        upper_treshold = data['revenue'].quantile(upper)\n",
    "        lower_treshold = data['revenue'].quantile(lower)\n",
    "        #stores.drop(stores[(stores['plaace_hierarchy_id']==store_type) & (stores['revenue']>upper_treshold)].index, inplace=True)\n",
    "        #stores.drop(stores[(stores['plaace_hierarchy_id']==store_type) & (stores['revenue']<lower_treshold)].index, inplace=True)\n",
    "        \n",
    "        stores.iloc[stores[(stores['plaace_hierarchy_id']==store_type) & (stores['revenue']>upper_treshold)].index,[col_idx]] = upper_treshold\n",
    "        stores.iloc[stores[(stores['plaace_hierarchy_id']==store_type) & (stores['revenue']<lower_treshold)].index,[col_idx]] = lower_treshold\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "quantile_storeType_vs_revenue(stores_train,0.05,0.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot after removing outliers\n",
    "\n",
    "you can see in the plot below that the outliers has been removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaace_hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "stores_with_hierarchy = stores_train.merge(plaace_hierarchy, how='left', on='plaace_hierarchy_id')\n",
    "for store_type in stores_with_hierarchy['lv2_desc'].unique():\n",
    "    plt.figure(figsize=(12,2))\n",
    "    sns.violinplot(x='lv3_desc',y='revenue',data=stores_with_hierarchy[stores_with_hierarchy[\"lv2_desc\"]==store_type]).set_title(f\"{store_type} violin plot\")\n",
    "    plt.show()\n",
    "    break #comment out for exploring more store types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "rev_log = pd.DataFrame()\n",
    "rev_log['revenue_log'] = np.log1p(stores_train['revenue'])\n",
    "\n",
    "rev_capped_log = stores_train.copy()\n",
    "quantile_storeType_vs_revenue(rev_capped_log, 0.00, 0.90)\n",
    "rev_capped_log['revenue'] = np.log1p(rev_capped_log['revenue'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(15, 5), ncols=2, dpi=100)\n",
    "sns.distplot(rev_log['revenue_log'], ax=ax1, bins=91);\n",
    "ax1.set_title('Distribution log transformed revenue');\n",
    "sns.distplot(rev_capped_log['revenue'], ax=ax2, bins=91);\n",
    "ax2.set_title('Distribution capped log transform revenue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantiling the data improved our predictions greatly in the beginning of the project, but we saw that it didnt have a noticable impact after we began exploring log transform, the plot above shows that quantiling does not improve distribution with log transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comparing test set to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "stores_test = pd.read_csv('data/stores_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9), dpi=600)\n",
    "plt.scatter(stores_train['lon'],stores_train['lat'], label=\"traing\",color='red')\n",
    "plt.scatter(stores_test['lon'], stores_test['lat'], alpha=0.2, label=\"test\", color=\"blue\")\n",
    "plt.legend(fontsize=10,ncol=2)\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200)\n",
    "ax1 = fig.add_subplot(projection='3d')\n",
    "ax1.scatter(stores_train['lon'],stores_train['lat'],stores_train['revenue'])\n",
    "ax1.set_xlabel('Lat')\n",
    "ax1.set_ylabel('Lon')\n",
    "ax1.set_zlabel('Revenue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine whether a store occurs in multiple datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_that_are_in_both_sets(df1, df2):\n",
    "    \n",
    "    duplicate_set = pd.merge(df1,df2, how='inner', on='store_name')\n",
    "    return duplicate_set\n",
    "\n",
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "stores_test = pd.read_csv('data/stores_test.csv')\n",
    "stores_extra = pd.read_csv('data/stores_extra.csv')\n",
    "\n",
    "dup = stores_that_are_in_both_sets(stores_test, stores_train)\n",
    "dup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the other data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss_stopps = pd.read_csv('data/busstops_norway.csv')\n",
    "buss_stopps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "grunnkrets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_incomes = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "gk_incomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "gk_households.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_ages = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "gk_ages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in stores_train:\n",
    "    print(stores_train[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________ _2. Feature Engineering_ ___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "gk_incomes = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "gk_households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "gk_ages = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "buss_stopps = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "grunnkrets.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "gk_incomes.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "gk_households.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "gk_ages.drop_duplicates(subset=['grunnkrets_id'], inplace=True)\n",
    "buss_stopps.drop_duplicates(subset=['busstop_id'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of people in grunnkrets\n",
    "gk_ages['tot_people'] = np.sum(gk_ages.iloc[:,np.arange(2,93,1)], axis=1)\n",
    "gk_ages.head()\n",
    "\n",
    "# people density (number of people divided by arekm2)\n",
    "gk_area = grunnkrets[['grunnkrets_id','area_km2']]\n",
    "gk_ages = pd.merge(gk_ages, gk_area, how='left', on='grunnkrets_id')\n",
    "gk_ages['people_density'] = (gk_ages['tot_people'] / gk_ages['area_km2'])\n",
    "gk_ages['people_density_log'] = np.log1p(gk_ages['tot_people'] / gk_ages['area_km2'])\n",
    "\n",
    "gk_ages.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of house holds\n",
    "gk_households['nb_households']  = np.sum(gk_households.iloc[:,np.arange(2,10,1)], axis=1)\n",
    "gk_households.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buss stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lat long extraction\n",
    "#buss_stopps.head()\n",
    "#buss_stopps['geometry'].head()\n",
    "'''\n",
    "for row in buss_stoppdafsfass\n",
    "\n",
    "'''\n",
    "\"\"\" string = \"POINT(10.7781327278563 59.9299988828761)\"\n",
    "#buss_stopps['geometry'] \n",
    "sliced = string[6:]\n",
    "sliced = sliced[:]\n",
    "print(sliced) \"\"\"\n",
    "\n",
    "\n",
    "## String fiksing\n",
    "\"\"\" string2 = \"POINT(10.7781327278563 59.9299988828761)\"\n",
    "string2 = string2[6:]\n",
    "string2 = string2.replace(')','')\n",
    "print(string2)\n",
    "lon = string2.split()[0]\n",
    "lat = string2.split()[1]\n",
    "print(lat, lon)\n",
    "\n",
    "lon = float(lon)\n",
    "lat = float(lat)\n",
    "print(type(lon), lon)\n",
    "print(type(lat), lat) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" def addLatAndLong(string):\n",
    "    string = string[6:]\n",
    "    string = string.replace(')','')\n",
    "    lon = string.split()[0]\n",
    "    lat = string.split()[1]\n",
    "    lon = float(lon)\n",
    "    lat = float(lat)    \n",
    "\n",
    "    return lat, lon \"\"\"\n",
    "\n",
    "#Funker bare med buss_stopps\n",
    "def addLatAndLong(buss):\n",
    "    #count = 0\n",
    "    buss['lat'] = 0.0\n",
    "    buss['lon'] = 0.0\n",
    "    lonList = []\n",
    "    latList = [] \n",
    "    for index, row in buss.iterrows():\n",
    "        lon = row['geometry']\n",
    "        lon = lon[6:]\n",
    "        lon = lon.replace(')','')\n",
    "        \n",
    "        lat = lon.split()[1]\n",
    "        lon = lon.split()[0]\n",
    "        lon = float(lon)\n",
    "        lat = float(lat) \n",
    "\n",
    "        lonList.append(lon)\n",
    "        latList.append(lat)\n",
    "\n",
    "        #buss[index]['lat'] = lat\n",
    "        #buss[index]['lon'] = lon\n",
    "        #row['lat'] = lat\n",
    "        #row['lon'] = lon\n",
    "\n",
    "        \"\"\"  \n",
    "        print(type(lon),lon, type(lat), lat )\n",
    "        count +=1\n",
    "        if count > 100:\n",
    "            break \n",
    "        \"\"\"\n",
    "    buss['lon'] = np.array(lonList)\n",
    "    buss['lat'] = np.array(latList)\n",
    "    #print(lonList)\n",
    "    #print(latList)\n",
    "\n",
    "#addLatAndLong(buss_stopps)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "#addLatAndLong(buss_stopps)\n",
    "#print(str(buss_stopps['geometry']))\n",
    "\n",
    "buss_stopps.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(mt.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = mt.sin(dlat/2)**2 + mt.cos(lat1) * mt.cos(lat2) * mt.sin(dlon/2)**2\n",
    "    c = 2 * mt.asin(mt.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss_stopps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Antall busstopp innafor x (km/m)\n",
    "# Antall busstopp innafor y (km/m)\n",
    "\n",
    "# Avstand til nærmeste busstopp\n",
    "\"\"\" def distBetweenStoreAndBuss(stores, buss):\n",
    "\n",
    "    quit = False\n",
    "\n",
    "    for row in stores.iterrows():\n",
    "        distlist = []\n",
    "        quit = True\n",
    "        lonS = float(stores['lon'])\n",
    "        latS = float(stores['lat'])\n",
    "        \n",
    "        lonS = float(lonS)\n",
    "        latS = float(latS) \n",
    "    \n",
    "        \n",
    "        for row in buss:\n",
    "            lonB = float(buss['lon'])\n",
    "            latB = float(buss['lat'])\n",
    "\n",
    "            distance = haversine(lonS, latS, lonB, latB)\n",
    "            print(distance)\n",
    "            #distlist.append(haversine(lonS, latS, lonB, latB))\n",
    "        if quit:\n",
    "            return \n",
    "    \n",
    "    print(distlist) \"\"\"\n",
    "def tester(dataframe):\n",
    "    lon = dataframe['lon'].values[0]\n",
    "    print(type(lon), lon)\n",
    "    #for index, row in dataframe.iterrows():\n",
    "    #    print(index, row['store_name'])\n",
    "\n",
    "#tester(buss_stopps)\n",
    "\n",
    "\"\"\" \n",
    "def distBetweenStoreAndBuss(stores, buss):\n",
    "\n",
    "    #lonBuss = float(5.89980086113255)\n",
    "    #latBuss = float(60.1421872817075)\n",
    "    \n",
    "    lonBuss = buss['lon'].values[0]\n",
    "    latBuss = buss['lat'].values[0]\n",
    "\n",
    "    stores['closestBusStop'] = np.zeros\n",
    "    stores['busStopsWithin2km'] = np.zeros\n",
    "    stores['busStopsWithin5km'] = np.zeros\n",
    "    stores['busStopsWithin10km'] = np.zeros\n",
    "    \n",
    "    tens = []\n",
    "    fives = []\n",
    "    twos = []\n",
    "    closestStops = []\n",
    "\n",
    "    count = 0\n",
    "    for index, row in stores.iterrows():\n",
    "        # Antall busstopp innafor x (km/m)\n",
    "        # Antall busstopp innafor y (km/m)\n",
    "\n",
    "        # Avstand til nærmeste busstopp\n",
    "        lonStore = float(row['lon'])\n",
    "        latStore = float(row['lat'])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        closestStop = np.Inf\n",
    "        print(\"current iteration: \", count)\n",
    "        for index, rad in buss.iterrows():\n",
    "            lonBuss = float(rad['lon'])\n",
    "            latBuss = float(rad['lat'])\n",
    "\n",
    "            dist = haversine(lonStore, latStore, lonBuss, latBuss)\n",
    "\n",
    "            ten = 0\n",
    "            five = 0\n",
    "            two = 0\n",
    "            if dist < float(10):\n",
    "                ten += 1\n",
    "                if dist < float(5):\n",
    "                    five += 1\n",
    "                    if dist < float(2):\n",
    "                        two += 1\n",
    "    \n",
    "            if closestStop > dist:\n",
    "                closestStop = dist\n",
    "        count += 1\n",
    "        tens.append(ten)\n",
    "        fives.append(five)\n",
    "        twos.append(two)\n",
    "        closestStops.append(closestStop)\n",
    "        #break\n",
    "\n",
    "    \n",
    "    stores['closestBusStop'] = np.array(closestStops)\n",
    "    stores['busStopsWithin2km'] = np.array(twos)\n",
    "    stores['busStopsWithin5km'] = np.array(fives)\n",
    "    stores['busStopsWithin10km'] = np.array(tens)\n",
    "\n",
    "    \n",
    "\"\"\"  \n",
    "\n",
    "def distBetweenStoreAndBuss(stores, buss):\n",
    "\n",
    "    #lonBuss = float(5.89980086113255)\n",
    "    #latBuss = float(60.1421872817075)\n",
    "    \n",
    "    lonBuss = buss['lon'].values[0]\n",
    "    latBuss = buss['lat'].values[0]\n",
    "    \n",
    "    \"\"\" \n",
    "    stores['closestBusStop'] = np.zeros\n",
    "    stores['busStopsWithin2km'] = np.zeros\n",
    "    stores['busStopsWithin5km'] = np.zeros\n",
    "    \"\"\"\n",
    "    stores['busStopsWithin10km'] = np.zeros\n",
    "    \n",
    "    tens = []\n",
    "    fives = []\n",
    "    twos = []\n",
    "    closestStops = []\n",
    "\n",
    "    count = 0\n",
    "    for index, row in stores.iterrows():\n",
    "        # Antall busstopp innafor x (km/m)\n",
    "        # Antall busstopp innafor y (km/m)\n",
    "\n",
    "        # Avstand til nærmeste busstopp\n",
    "        lonStore = float(row['lon'])\n",
    "        latStore = float(row['lat'])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        closestStop = np.Inf\n",
    "        print(\"current iteration: \", count)\n",
    "        for index, rad in buss.iterrows():\n",
    "            lonBuss = float(rad['lon'])\n",
    "            latBuss = float(rad['lat'])\n",
    "\n",
    "            dist = haversine(lonStore, latStore, lonBuss, latBuss)\n",
    "\n",
    "            ten = 0\n",
    "            five = 0\n",
    "            two = 0\n",
    "            if dist < float(10):\n",
    "                ten += 1\n",
    "            \"\"\"    \n",
    "                if dist < float(5):\n",
    "                    five += 1\n",
    "                    if dist < float(2):\n",
    "                        two += 1\n",
    "    \n",
    "            if closestStop > dist:\n",
    "                closestStop = dist \n",
    "            \"\"\"\n",
    "        count += 1\n",
    "        tens.append(ten)\n",
    "        \"\"\" \n",
    "        fives.append(five)\n",
    "        twos.append(two)\n",
    "        closestStops.append(closestStop) \n",
    "        \"\"\"\n",
    "        #break\n",
    "\n",
    "    \"\"\" \n",
    "    stores['closestBusStop'] = np.array(closestStops)\n",
    "    stores['busStopsWithin2km'] = np.array(twos)\n",
    "    stores['busStopsWithin5km'] = np.array(fives)\n",
    "    \"\"\"\n",
    "    stores['busStopsWithin10km'] = np.array(tens)\n",
    "\n",
    "    \"\"\" \n",
    "    stores['closestBusStop'] = np.array(closestStop)\n",
    "    buss['lat'] = np.array(latList) \n",
    "    \"\"\"\n",
    "\n",
    "distBetweenStoreAndBuss(stores_train, buss_stopps)\n",
    "      \n",
    "#        for row in buss:\n",
    "#            lonB = float(buss['lon'])\n",
    "#            latB = float(buss['lat'])\n",
    "#            distance = haversine(lonS, latS, lonB, latB)\n",
    "#            print(distance)\n",
    "#            #distlist.append(haversine(lonS, latS, lonB, latB))\n",
    "#        if quit:\n",
    "#            return \n",
    "    \n",
    "#    pr\n",
    "# int(distlist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "#Buss\n",
    "latBuss = 60.1421872817075\n",
    "lonBuss = 5.89980086113255\n",
    "\n",
    "#store\n",
    "latStore = 59.9137594158249\n",
    "lonStore = 10.7340307646896\n",
    "\n",
    "print(haversine(latBuss, lonBuss, latStore, lonStore))\n",
    "\n",
    "\n",
    "stores_train.head() \n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(X, Y):\n",
    "    sx = np.sum(X**2, keepdims=True)\n",
    "    #sx = X*X.T\n",
    "    sy = np.sum(Y**2, axis=1, keepdims=True)\n",
    "    return np.sqrt(-2 * X.dot(Y.T) + sx + sy.T)\n",
    "\n",
    "busMatrix = buss_stopps[['lat', 'lon']]\n",
    "busMatrix = busMatrix.to_numpy()\n",
    "\n",
    "storeMatrix = stores_train[['lat', 'lon']]\n",
    "storeMatrix = storeMatrix.to_numpy()\n",
    "\n",
    "#storeMatrix = pd.DataFrame(stores_train[['lat', 'lon']])\n",
    "\n",
    "\n",
    "for row in storeMatrix: \n",
    "    #print(row)\n",
    "    result = dist(row, busMatrix)\n",
    "\n",
    "    #print(type(result))\n",
    "    #result = np.array(result)\n",
    "    #print(result.shape)\n",
    "    result = result.flatten() \n",
    "\n",
    "    print(result[result<0.0015])\n",
    "\n",
    "    #print(min(result))\n",
    "    \n",
    "\n",
    "\n",
    "#busMatrix = busMatrix.T\n",
    "\n",
    "\n",
    "\"\"\" print(storeMatrix.shape)\n",
    "print(busMatrix.shape) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#newMatrix = dist(storeMatrix, busMatrix)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tikmfil = pd.DataFrame()\n",
    "tikmfil['id'] = test_data['store_id']\n",
    "#submission['busStopsWithin10km'] = np.asarray(pred_data)\n",
    "tikmfil['busStopsWithin10km'] = stores_train['busStopsWithin10km']\n",
    "tikmfil.to_csv('submissionFiles\\\\BusstoppTikmFil.csv', index=False)\n",
    "tikmfil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_aggregate_columns (stores):\n",
    "    # has mall\n",
    "\n",
    "    # has chain\n",
    "\n",
    "    # Distance to another store\n",
    "\n",
    "    # Distance to another store of same type\n",
    "\n",
    "    # Density of stores in grunnkrets\n",
    "    #gk_area = grunnkrets[['grunnkrets_id','area_km2']]\n",
    "    #st_dens = pd.merge(gk_ages, gk_area, how='left', on='grunnkrets_id')\n",
    "\n",
    "    # lat lon log transform\n",
    "    stores['lat_log'] = np.log1p(stores['lat'])\n",
    "    stores['lon_log'] = np.log1p(stores['lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_selected_columns(df):\n",
    "    self_aggregate_columns(df)\n",
    "    gk = grunnkrets[['grunnkrets_id','municipality_name']]\n",
    "    gk_i = gk_incomes[['grunnkrets_id','all_households']] #all house holds = median income\n",
    "    gk_h = gk_households[['grunnkrets_id','nb_households']]\n",
    "    gk_a = gk_ages[['grunnkrets_id','tot_people','people_density_log']]\n",
    "    \n",
    "    concat = pd.merge(df, gk, how='left', on='grunnkrets_id')\n",
    "    concat = pd.merge(concat, gk_i, how='left', on='grunnkrets_id')\n",
    "    concat = pd.merge(concat, gk_h, how='left', on='grunnkrets_id')\n",
    "    concat = pd.merge(concat, gk_a, how='left', on='grunnkrets_id')\n",
    "    \n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "remove_columns(stores_train, ['store_id','store_name','year','address','sales_channel_name'])\n",
    "stores_train = add_selected_columns(stores_train)\n",
    "sns.heatmap(stores_train.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________ _3. Machine Learning Models and Predictions_ ___________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_DType_LGBM(dFrame):\n",
    "    le = LabelEncoder()\n",
    "    X = pd.DataFrame()\n",
    "    \n",
    "    for col_name in dFrame:\n",
    "        if dFrame[col_name].dtypes == 'object':\n",
    "            X[col_name] = dFrame[col_name].astype('category')\n",
    "            \n",
    "        #elif col_name == 'grunnkrets_id':\n",
    "        #    X[col_name] = le.fit_transform(dFrame[col_name])\n",
    "        \n",
    "        else:\n",
    "            X[col_name] = dFrame[col_name]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_DType_CatBoost(dFrame):\n",
    "    le = LabelEncoder()\n",
    "    X = pd.DataFrame()\n",
    "    for col_name in dFrame:\n",
    "        \n",
    "        if col_name == 'grunnkrets_id':# or col_name == 'plaace_hierarchy_id':\n",
    "            #X[col_name] = dFrame[col_name]\n",
    "            X[col_name] = le.fit_transform(dFrame[col_name])\n",
    "            #X[col_name] = dFrame[col_name].astype(str)\n",
    "        \n",
    "        elif dFrame[col_name].dtypes == 'object':\n",
    "            X[col_name] = dFrame[col_name].astype(str)\n",
    "            \n",
    "        else:\n",
    "            X[col_name] = dFrame[col_name]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train data and divide into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(test_size=0.20):\n",
    "    stores_train = pd.read_csv('data/stores_train.csv')\n",
    "    stores_test = pd.read_csv('data/stores_test.csv')\n",
    "\n",
    "    # select prefered columns\n",
    "    remove_columns(stores_train, ['store_id','store_name','year','address','sales_channel_name'])\n",
    "    remove_columns( stores_test, ['store_id','store_name','year','address','sales_channel_name'])\n",
    "\n",
    "    # Add features\n",
    "    #print(stores_train.shape) \n",
    "    stores_train = add_selected_columns(stores_train)\n",
    "    stores_test = add_selected_columns(stores_test)\n",
    "    #print(stores_train.shape)\n",
    "\n",
    "    # Preprocess/Clean data\n",
    "    #quantile_storeType_vs_revenue(stores_train,0.01, 0.88)\n",
    "\n",
    "    # Divide data into train and test set\n",
    "    x_train = stores_train.drop('revenue', axis=1)\n",
    "\n",
    "    y_train = stores_train['revenue']\n",
    "    y_train=np.log1p(y_train) #log transform revenue\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=test_size, random_state=3)\n",
    "    \n",
    "    return  x_train, x_test, y_train, y_test, stores_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_check, _, _, _, _ = get_data()\n",
    "x_train_check.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "LGBM_x_train, LGBM_x_test, y_train, y_test, _ = get_data()\n",
    "\n",
    "# Convert to approperiate dtypes\n",
    "LGBM_x_train = convert_DType_LGBM(LGBM_x_train)\n",
    "LGBM_x_test = convert_DType_LGBM(LGBM_x_test)\n",
    "\n",
    "\n",
    "\n",
    "# Make model, fit and predict\n",
    "parameters = {# Params obtained trough testing and reading up on this guide: https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n",
    "              #'metric': 'acc',\n",
    "              #'n_estimators' : 400\n",
    "              #'path_smooth' : 0.5,\n",
    "              #'min_data_in_leaf' : 3\n",
    "}\n",
    "\n",
    "LGBM_model = LGBMRegressor(**parameters)\n",
    "LGBM_model.fit(LGBM_x_train, y_train)\n",
    "LGBM_pred = LGBM_model.predict(LGBM_x_test)\n",
    "LGBM_pred=np.expm1(LGBM_pred) #invert log transform\n",
    "\n",
    "# Run some tests\n",
    "number_of_negatives = 0\n",
    "for i in range(len(LGBM_pred)):\n",
    "    if LGBM_pred[i] < 0.0:\n",
    "        number_of_negatives += 1\n",
    "        LGBM_pred[i] = 0.0\n",
    "\n",
    "print(f\"number of negatives: {number_of_negatives}\")\n",
    "print(f\"rmsle: {rmsle(y_test,LGBM_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous rmse scores gave the following kaggle scores:\n",
    "# - 0.9055645241057166 rmsle resulted in: 0.71576 on kaggle - LGBM a lot of columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "CB_x_train, CB_x_test, y_train, y_test, _ = get_data()\n",
    "\n",
    "# Convert to approperiate dtypes\n",
    "CB_x_train = convert_DType_CatBoost(CB_x_train)\n",
    "CB_x_test = convert_DType_CatBoost(CB_x_test)\n",
    "categorical_features_indices = np.where((CB_x_train.dtypes != np.float))[0]\n",
    "\n",
    "print(CB_x_train.dtypes)\n",
    "\n",
    "# Make model, fit and predict\n",
    "parameters = {\n",
    "    #some param\n",
    "}\n",
    "\n",
    "CB_model = cb.CatBoostRegressor(loss_function='RMSE', **parameters)\n",
    "CB_model.fit(CB_x_train,y_train, cat_features=categorical_features_indices)\n",
    "CB_pred = CB_model.predict(CB_x_test)\n",
    "CB_pred = np.expm1(CB_pred)\n",
    "\n",
    "\n",
    "# Run some tests\n",
    "number_of_negatives = 0\n",
    "for i in range(len(CB_pred)):\n",
    "    if CB_pred[i] < 0.0:\n",
    "        number_of_negatives += 1\n",
    "        CB_pred[i] = 0.0\n",
    "\n",
    "print(f\"number of negatives: {number_of_negatives}\")\n",
    "print(f\"rmsle: {rmsle(y_test,CB_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_preds = pd.DataFrame()\n",
    "compare_preds['true'] = y_test\n",
    "compare_preds['mean'] = (CB_pred+LGBM_pred)/2\n",
    "compare_preds['catboost'] = CB_pred\n",
    "compare_preds['CB err'] = np.abs(CB_pred - y_test)\n",
    "compare_preds['lightgbm'] = LGBM_pred\n",
    "compare_preds['LGBM err'] = np.abs(LGBM_pred - y_test)\n",
    "\n",
    "print(f\" CB err sum: {np.sum(compare_preds['CB err'])}\")\n",
    "print(f\" CB err mean: {np.mean(compare_preds['CB err'])}\")\n",
    "print(f\" LGBM err sum: {np.sum(compare_preds['LGBM err'])}\")\n",
    "print(f\" LGBM err mean: {np.mean(compare_preds['LGBM err'])}\")\n",
    "\n",
    "\n",
    "#compare_preds.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train, _, y_train, _, test = get_data(test_size=0.01)\n",
    "\n",
    "# Convert to approperiate dtypes\n",
    "LGBM_x_train = convert_DType_LGBM(x_train)\n",
    "LGBM_test = convert_DType_LGBM(test)\n",
    "\n",
    "CB_x_train = convert_DType_CatBoost(x_train)\n",
    "CB_test = convert_DType_CatBoost(test)\n",
    "categorical_features_indices = np.where((CB_x_train.dtypes != np.float))[0]\n",
    "\n",
    "# LGBM\n",
    "LGBM_model = LGBMRegressor(**parameters)\n",
    "LGBM_model.fit(LGBM_x_train, y_train)\n",
    "LGBM_pred = LGBM_model.predict(LGBM_test)\n",
    "LGBM_pred=np.expm1(LGBM_pred) #invert log transform\n",
    "\n",
    "# Catboost\n",
    "CB_model = cb.CatBoostRegressor(loss_function='RMSE', **parameters, silent=True)\n",
    "CB_model.fit(CB_x_train,y_train, cat_features=categorical_features_indices)\n",
    "CB_pred = CB_model.predict(CB_test)\n",
    "CB_pred = np.expm1(CB_pred)\n",
    "\n",
    "# Aggregate result\n",
    "PREDICTION = LGBM_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#write the predicition to file\n",
    "writeResultToFile(stores_test, PREDICTION, \"LGMB\")\n",
    "\n",
    "# Verify format of submission file\n",
    "submissionVery = pd.read_csv('submissionFiles/LGMB.csv')\n",
    "submissionVery.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "stores_test = pd.read_csv('data/stores_test.csv')\n",
    "test = stores_test.copy()\n",
    "\n",
    "# select prefered columns\n",
    "remove_columns(stores_train, ['store_id','store_name','year','address','sales_channel_name'])\n",
    "remove_columns(test, ['store_id','store_name','year','address','sales_channel_name'])\n",
    "\n",
    "# Add features\n",
    "print(stores_train.shape, test.shape)\n",
    "stores_train = add_selected_columns(stores_train)\n",
    "test = add_selected_columns(test)\n",
    "print(stores_train.shape, test.shape)\n",
    "\n",
    "# Preprocess/Clean data\n",
    "#quantile_storeType_vs_revenue(stores_train,0.01, 0.88)\n",
    "\n",
    "# Divide data into train and test set\n",
    "x_train = stores_train.drop('revenue', axis=1)\n",
    "y_train = stores_train['revenue']\n",
    "y_train=np.log1p(y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(x_train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_x_train = convert_DType_LGBM(x_train)\n",
    "lgbm_test = convert_DType_LGBM(test)\n",
    "# Model, fit and predict\n",
    "LGBM_model =LGBMRegressor(**parameters)\n",
    "LGBM_model.fit(lgbm_x_train, y_train)\n",
    "submission_pred = LGBM_model.predict(lgbm_test)\n",
    "submission_pred=np.expm1(submission_pred)\n",
    "# remove negative values\n",
    "number_of_negatives = 0\n",
    "for i in range(len(submission_pred)):\n",
    "    if submission_pred[i] < 0.0:\n",
    "        number_of_negatives += 1\n",
    "        submission_pred[i] = 0.0\n",
    "print(f\"number of negatives: {number_of_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the predicition to file\n",
    "writeResultToFile(stores_test, submission_pred, \"LGBM_plaace_hierarchy_id_grunnkrets_id_lat_lon_chain_name_mall_name_lat_log_lon_log_municipality_name_all_households_nb_households_tot_people_people_density_log\")\n",
    "\n",
    "# Verify format of submission file\n",
    "submissionVery = pd.read_csv('submissionFiles/LGBM_plaace_hierarchy_id_grunnkrets_id_lat_lon_chain_name_mall_name_lat_log_lon_log_municipality_name_all_households_nb_households_tot_people_people_density_log.csv')\n",
    "submissionVery.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost pred and submitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "stores_test = pd.read_csv('data/stores_test.csv')\n",
    "test = stores_test.copy()\n",
    "\n",
    "# select prefered columns\n",
    "remove_columns(stores_train, ['store_id','store_name','year','address','sales_channel_name'])\n",
    "remove_columns(test, ['store_id','store_name','year','address','sales_channel_name'])\n",
    "\n",
    "# Add features\n",
    "print(stores_train.shape, test.shape)\n",
    "stores_train = add_selected_columns(stores_train)\n",
    "test = add_selected_columns(test)\n",
    "print(stores_train.shape, test.shape)\n",
    "\n",
    "# Preprocess/Clean data\n",
    "#quantile_storeType_vs_revenue(stores_train,0.01, 0.88)\n",
    "\n",
    "# Divide data into train and test set\n",
    "x_train = stores_train.drop('revenue', axis=1)\n",
    "y_train = stores_train['revenue']\n",
    "y_train=np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_x_train = x_train\n",
    "categorical_features_indices = np.where((cb_x_train.dtypes != np.float))[0]\n",
    "cb_x_train = convert_DType_CatBoost(cb_x_train)\n",
    "\n",
    "cb_x_test = test\n",
    "cb_x_test = convert_DType_CatBoost(cb_x_test)\n",
    "\n",
    "CB_model = cb.CatBoostRegressor(loss_function='RMSE')\n",
    "grid = {'iterations': [100, 150, 200],\n",
    "        'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 4, 6, 8],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "\n",
    "CB_model.fit(cb_x_train,y_train, cat_features=categorical_features_indices)\n",
    "CB_pred = CB_model.predict(cb_x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the predicition to file\n",
    "writeResultToFile(stores_test, CB_pred, \"Cat_first\")\n",
    "\n",
    "# Verify format of submission file\n",
    "submissionVery = pd.read_csv('submissionFiles/Cat_first.csv')\n",
    "submissionVery.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _____ Random Forest Regressor _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load, preprocess and convert data to correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "stores_test = pd.read_csv('data/stores_test.csv')\n",
    "\n",
    "# Preprocess/Clean data\n",
    "remove_columns(stores_train, ['store_id','year','store_name','sales_channel_name','address','chain_name','mall_name'])\n",
    "remove_columns(stores_test, ['store_id','year','store_name','sales_channel_name','address','chain_name','mall_name'])\n",
    "#remove_retailers_with_0_revenue(stores_train)\n",
    "quantile_storeType_vs_revenue(stores_train,0.10, 0.80)\n",
    "\n",
    "# Divide data into x and y train\n",
    "x_train = stores_train.drop('revenue', axis=1)\n",
    "y_train = stores_train['revenue']\n",
    "x_test = stores_test.copy()\n",
    "\n",
    "# Convert from object type to numerical\n",
    "#train set\n",
    "cat_columns = x_train.select_dtypes(['object']).columns\n",
    "x_train[cat_columns] = x_train[cat_columns].apply(lambda x: pd.factorize(x)[0])\n",
    "#test set\n",
    "cat_columns = x_test.select_dtypes(['object']).columns\n",
    "x_test[cat_columns] = x_test[cat_columns].apply(lambda x: pd.factorize(x)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "RFR = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Fitting\n",
    "RFR.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test RFR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the training data set as a pin pointer\n",
    "pred_train_RFR = RFR.predict(x_train)\n",
    "print(rmsle(y_train, pred_train_RFR))\n",
    "print(RFR.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict test and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_RFR = RFR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "#writeResultToFile(stores_test, pred_test_RFR, \"RFR_10_80_percentile\")\n",
    "\n",
    "# Verify format of submission file\n",
    "#submissionVery = pd.read_csv('submissionFiles/RFR_10_80_percentile.csv')\n",
    "#submissionVery.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emil modeller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pythons stuff emil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = x_train.select_dtypes(['category']).columns\n",
    "x_train[cat_columns] = x_train[cat_columns].apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "# Tune\n",
    "tuner = LGBMTuner(metric = 'rmsle', verbosity=0)\n",
    "tuner.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lime stuff in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "\n",
    "tuner.plot_importances()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final improved models/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return metrics.mean_squared_log_error(y_true, y_pred)**0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
